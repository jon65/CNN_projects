{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO - Define ResNet18 model.\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Layer, BatchNormalization, ReLU, Add, GlobalAvgPool2D, Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class BasicBlock_withResidule(Layer):\n",
    "    def __init__(self, filters: int, strides: int, downsample=False):\n",
    "        super().__init__()\n",
    "        self.conv0 = Conv2D(filters, kernel_size=3, padding=\"same\", strides=strides)\n",
    "        self.bn0 = BatchNormalization()\n",
    "        self.relu0 = ReLU()\n",
    "\n",
    "        self.conv1 = Conv2D(filters, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.add = Add()\n",
    "        self.relu1 = ReLU()\n",
    "        if downsample and strides != 1:\n",
    "          # self.downsample = Conv2D(filters, kernel_size=1, strides=strides)\n",
    "          self.downsample = Sequential([Conv2D(filters, kernel_size=1, strides=strides),\n",
    "                                        BatchNormalization()])\n",
    "\n",
    "        else:\n",
    "          self.downsample = Layer() # identity layer\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        identity = self.downsample(inputs)\n",
    "        output = self.add([identity, x])\n",
    "\n",
    "        output = self.relu1(output)\n",
    "        return output\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classess=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(64, 7, strides=2, padding=\"same\")\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPooling2D(3, strides=2, padding=\"same\")\n",
    "\n",
    "        num_block = 2\n",
    "\n",
    "        # Longer version of code\n",
    "        conv2_x_block = []\n",
    "        for idx_b in range(num_block):\n",
    "          conv2_x_block.append(BasicBlock_withResidule(filters=64, strides=1, downsample=False))\n",
    "        self.conv2_x = Sequential(conv2_x_block)\n",
    "\n",
    "        conv3_x_block = []\n",
    "        for idx_b in range(num_block):\n",
    "          conv3_x_block.append(BasicBlock_withResidule(filters=128, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False))\n",
    "        self.conv3_x = Sequential(conv3_x_block)\n",
    "\n",
    "        conv4_x_block = []\n",
    "        for idx_b in range(num_block):\n",
    "          conv4_x_block.append(BasicBlock_withResidule(filters=256, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False))\n",
    "        self.conv4_x = Sequential(conv4_x_block)\n",
    "\n",
    "        conv5_x_block = []\n",
    "        for idx_b in range(num_block):\n",
    "          conv5_x_block.append(BasicBlock_withResidule(filters=512, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False))\n",
    "        self.conv5_x = Sequential(conv5_x_block)\n",
    "\n",
    "        # Shorter version of code\n",
    "        # self.conv2_x = Sequential([BasicBlock_withResidule(filters=64, strides=1, downsample=False)])\n",
    "        # self.conv3_x = Sequential([BasicBlock_withResidule(filters=128, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False) for idx_b in range(num_block)])\n",
    "        # self.conv4_x = Sequential([BasicBlock_withResidule(filters=256, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False) for idx_b in range(num_block)])\n",
    "        # self.conv5_x = Sequential([BasicBlock_withResidule(filters=512, strides=2 if idx_b == 0 else 1, downsample=True if idx_b == 0 else False) for idx_b in range(num_block)])\n",
    "\n",
    "\n",
    "        self.global_pool = GlobalAvgPool2D()\n",
    "        self.fc = Dense(5)\n",
    "\n",
    "        self.build(input_shape=(None, 224, 224, 3))\n",
    "        self.call(Input(shape=(224, 224, 3)))\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jon/Documents/CNN_projects/tenserflow_env/lib/python3.11/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'res_net18_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/jon/Documents/CNN_projects/tenserflow_env/lib/python3.11/site-packages/keras/src/layers/layer.py:1295: UserWarning: Layer 'basic_block_with_residule_16' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Exception encountered when calling Layer.call().\n",
      "\n",
      "\u001b[1mLayer Layer does not have a `call()` method implemented.\u001b[0m\n",
      "\n",
      "Arguments received by Layer.call():\n",
      "  • args=('tf.Tensor(shape=(None, 56, 56, 64), dtype=float32)',)\n",
      "  • kwargs=<class 'inspect._empty'>''\n",
      "  warnings.warn(\n",
      "/Users/jon/Documents/CNN_projects/tenserflow_env/lib/python3.11/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'basic_block_with_residule_16', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling BasicBlock_withResidule.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'basic_block_with_residule_16' (of type BasicBlock_withResidule). Either the `BasicBlock_withResidule.call()` method is incorrect, or you need to implement the `BasicBlock_withResidule.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling Layer.call().\n\n\u001b[1mLayer Layer does not have a `call()` method implemented.\u001b[0m\n\nArguments received by Layer.call():\n  • args=('tf.Tensor(shape=(None, 56, 56, 64), dtype=float32)',)\n  • kwargs=<class 'inspect._empty'>\u001b[0m\n\nArguments received by BasicBlock_withResidule.call():\n  • args=('<KerasTensor shape=(None, 56, 56, 64), dtype=float32, sparse=None, name=keras_tensor_17>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m (train_images, train_labels), (test_images, test_labels) \u001b[38;5;241m=\u001b[39m cifar100\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     14\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 15\u001b[0m resnet_18 \u001b[38;5;241m=\u001b[39m \u001b[43mResNet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m resnet_18\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                   loss\u001b[38;5;241m=\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# reduce learning rate when the loss is not decreasing after 3 continuous epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mResNet18.__init__\u001b[0;34m(self, num_classess)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m, in \u001b[0;36mResNet18.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[1;32m     89\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[0;32m---> 91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3_x(x)\n\u001b[1;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4_x(x)\n",
      "File \u001b[0;32m~/Documents/CNN_projects/tenserflow_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mBasicBlock_withResidule.call\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m---> 34\u001b[0m identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd([identity, x])\n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(output)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling BasicBlock_withResidule.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'basic_block_with_residule_16' (of type BasicBlock_withResidule). Either the `BasicBlock_withResidule.call()` method is incorrect, or you need to implement the `BasicBlock_withResidule.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling Layer.call().\n\n\u001b[1mLayer Layer does not have a `call()` method implemented.\u001b[0m\n\nArguments received by Layer.call():\n  • args=('tf.Tensor(shape=(None, 56, 56, 64), dtype=float32)',)\n  • kwargs=<class 'inspect._empty'>\u001b[0m\n\nArguments received by BasicBlock_withResidule.call():\n  • args=('<KerasTensor shape=(None, 56, 56, 64), dtype=float32, sparse=None, name=keras_tensor_17>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# TODO - Compile the model with optimizer, loss and metrics\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Keras offers plenty of callbacks function\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "cifar100 = tf.keras.datasets.cifar100 \n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "\n",
    "epochs = 50\n",
    "resnet_18 = ResNet18()\n",
    "resnet_18.compile(optimizer=\"adam\",\n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# reduce learning rate when the loss is not decreasing after 3 continuous epochs\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.5, patience=3, verbose=2, min_lr=1e-8\n",
    ")\n",
    "history = resnet_18.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=epochs, callbacks=[reduce_lr_callback])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenserflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
